# cache_manager.py

import logging
import json
import gzip
from datetime import datetime
from typing import Dict, Any, Optional
from redis.asyncio import Redis as AsyncRedis
from urllib.parse import urlparse

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –í–°–Å –∏–∑ config.py
from config import (
    UPSTASH_REDIS_URL,
    UPSTASH_REDIS_TOKEN,
    WORKER_LOCK_KEY,
    WORKER_LOCK_VALUE
)

logger = logging.getLogger(__name__)
_redis_pool: Optional[AsyncRedis] = None


async def get_redis_connection() -> Optional[AsyncRedis]:
    """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis - –∏—Å–ø–æ–ª—å–∑—É–µ–º URL + TOKEN –æ—Ç–¥–µ–ª—å–Ω–æ"""
    global _redis_pool
    
    if _redis_pool is None:
        try:
            # –ü–∞—Ä—Å–∏–º URL —á—Ç–æ–±—ã –¥–æ—Å—Ç–∞—Ç—å —Ö–æ—Å—Ç –∏ –ø–æ—Ä—Ç
            parsed = urlparse(UPSTASH_REDIS_URL)
  
            _redis_pool = AsyncRedis(
                host=parsed.hostname,
                port=parsed.port or 6379,
                password=UPSTASH_REDIS_TOKEN,  # <-- –¢–æ–∫–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ!
                ssl=True,  # Upstash –≤—Å–µ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç SSL
                decode_responses=False,
                socket_connect_timeout=120, 
                socket_timeout=120,                 
                socket_keepalive=True
            )
 
            await _redis_pool.ping()
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Redis")
            return _redis_pool
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Redis: {e}")
            _redis_pool = None
            return None
    
    return _redis_pool


async def check_redis_health() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Redis."""
    conn = await get_redis_connection()
    if conn:
        try:
            return await conn.ping()
        except Exception:
            return False
    return False


async def load_from_cache(key: str, redis_conn: AsyncRedis) -> Optional[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Redis –ø–æ –∫–ª—é—á—É. –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç JSON."""
    cache_key = f"cache:{key}"
    data_bytes = await redis_conn.get(cache_key)
    
    if data_bytes:
        try:
            # --- –°–Ω–∞—á–∞–ª–∞ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º ---
            data_bytes = gzip.decompress(data_bytes)
            data_str = data_bytes.decode('utf-8')
            return json.loads(data_str)
        except (IOError, gzip.BadGzipFile, json.JSONDecodeError, UnicodeDecodeError) as e:
            # --- –û–±—Ä–∞–±–æ—Ç–∫–∞, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–∂–∞—Ç—ã (—Å—Ç–∞—Ä—ã–π –∫—ç—à) ---
            logger.warning(f"[CACHE] –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å gzip –¥–ª—è {cache_key} (–≤–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–∞—Ä—ã–π –∫—ç—à? –û—à–∏–±–∫–∞: {e}). –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω—ã–π JSON...")
            try:
                # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω—ã–π (–Ω–µ —Å–∂–∞—Ç—ã–π) JSON
                data_str = data_bytes.decode('utf-8')
                return json.loads(data_str)
            except Exception as e_inner:
                logger.error(f"[CACHE] –û—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª—é—á–∞ {cache_key} (–¥–∞–∂–µ –∫–∞–∫ fallback): {e_inner}")
                return None
    return None


async def load_raw_bytes_from_cache(key: str, redis_conn: AsyncRedis) -> Optional[bytes]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—ã—Ä—ã–µ –±–∞–π—Ç—ã –∏–∑ Redis (–±–µ–∑ –¥–µ–∫–æ–º–ø—Ä–µ—Å—Å–∏–∏).
    """
    cache_key = f"cache:{key}"
    # –ü—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–π—Ç—ã (—ç—Ç–æ GZIP), –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–∂–∞—Ç –≤ Redis
    return await redis_conn.get(cache_key)


async def save_to_cache(
    redis_conn: AsyncRedis, 
    key: str, 
    data: Dict[str, Any], 
    expiry_seconds: Optional[int] = None
) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ Redis –¢–û–õ–¨–ö–û –í GZIP-—Ñ–æ—Ä–º–∞—Ç–µ.
    """
    cache_key = f"cache:{key}"
    
    # –î–æ–±–∞–≤–ª—è–µ–º audit –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    if 'audit' not in data:
        data_content = data.get('data')
        count = 0
        if isinstance(data_content, (list, dict)):
            count = len(data_content)
            
        data['audit'] = {
            "timestamp": int(datetime.now().timestamp() * 1000),
            "source": "data_collector",
            "count": count
        }
        
    try:
        # 1. –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤ JSON
        data_json = json.dumps(data)
        data_bytes = data_json.encode('utf-8')
        
        # 2. –í–°–ï–ì–î–ê —Å–∂–∏–º–∞–µ–º
        compressed_data = gzip.compress(data_bytes, compresslevel=6)
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - GZIP
        if not compressed_data.startswith(b'\x1f\x8b'):
            logger.error(f"[CACHE] ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: gzip.compress() –Ω–µ –≤–µ—Ä–Ω—É–ª GZIP!")
            return False
        
        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Redis
        if expiry_seconds:
            result = await redis_conn.set(cache_key, compressed_data, ex=expiry_seconds)
        else:
            result = await redis_conn.set(cache_key, compressed_data)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        final_count = data.get('audit', {}).get('symbols_in_final_list', 'N/A')
        compression_ratio = (1 - len(compressed_data) / len(data_bytes)) * 100
        
        logger.info(
            f"[CACHE] ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {final_count} –∑–∞–ø–∏—Å–µ–π –≤ {cache_key}\n"
            f"        üì¶ –†–∞–∑–º–µ—Ä: {len(data_bytes):,} -> {len(compressed_data):,} –±–∞–π—Ç "
            f"({compression_ratio:.1f}% —Å–∂–∞—Ç–∏–µ)"
        )
        
        return result
        
    except Exception as e:
        logger.error(f"[CACHE] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {cache_key}: {e}", exc_info=True)
        return False


async def get_worker_status(redis_conn: AsyncRedis) -> Optional[bytes]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –≤–æ—Ä–∫–µ—Ä–∞ (bytes –∏–ª–∏ None)."""
    return await redis_conn.get(WORKER_LOCK_KEY)


async def check_if_task_is_running(timeframe: str, redis_conn: AsyncRedis) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ª–∏ –∑–∞–¥–∞—á–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ timeframe."""
    lock_status = await get_worker_status(redis_conn)
    return lock_status and lock_status.decode('utf-8') == WORKER_LOCK_VALUE